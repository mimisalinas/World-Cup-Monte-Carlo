Times:

10 simulations: 0m0.032s
100 simulations: 0m0.029s
1000 simulations: 0m0.033s
10000 simulations: 0m0.093s
100000 simulations: 0m0.717s
1000000 simulations:0m6.726s

Questions:

Which predictions, if any, proved incorrect as you increased the number of simulations?

It was predicted that the time will increase proportionally to the number of simulations.
However, seems like for a small number of simulations, the red time is much larger than the simulation time.
Therefore, is not until a much larger amount of simulations that we start seeing an increase in time.


Suppose you're charged a fee for each second of compute time your program uses.
After how many simulations would you call the predictions "good enough"?: TODO